<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>NLP on Rs&#39; Log</title>
    <link>http://localhost:1313/my-blog/tags/nlp/</link>
    <description>Recent content in NLP on Rs&#39; Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 09 Oct 2024 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/my-blog/tags/nlp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>大模型post-training方法</title>
      <link>http://localhost:1313/my-blog/posts/llm-post-training/</link>
      <pubDate>Wed, 09 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/my-blog/posts/llm-post-training/</guid>
      <description>&lt;p&gt;这篇主要从方法上梳理一下目前大模型后训练（post-training）的方法&lt;/p&gt;
&lt;h3 id=&#34;bradley-terry-model&#34;&gt;Bradley-Terry model&lt;/h3&gt;
&lt;p&gt;这是一个使用Bradley-Terry模型的示例公式：&lt;/p&gt;
&lt;p&gt;$$
P(i &amp;gt; j) = \frac{e^{\beta_i}}{e^{\beta_i} + e^{\beta_j}}
$$&lt;/p&gt;
&lt;p&gt;其中 ( \beta_i ) 和 ( \beta_j ) 分别是两个实体的参数。&lt;/p&gt;
&lt;h3 id=&#34;rlhf&#34;&gt;RLHF&lt;/h3&gt;
&lt;p&gt;在强化学习从人类反馈(RLHF)中，我们常常使用以下形式的优化方程：&lt;/p&gt;
&lt;p&gt;$$
\min_{\theta} \mathbb{E}\left[ \text{Loss}(\theta) \right]
$$&lt;/p&gt;
&lt;p&gt;这里 ( \theta ) 表示模型参数。&lt;/p&gt;
&lt;p&gt;这是一个测试样例&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>

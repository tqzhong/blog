<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Rs&#39; Log</title>
    <link>https://tqzhong.github.io/my-blog/posts/</link>
    <description>Recent content in Posts on Rs&#39; Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 09 Oct 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://tqzhong.github.io/my-blog/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>大模型post-training方法</title>
      <link>https://tqzhong.github.io/my-blog/posts/blog1/</link>
      <pubDate>Wed, 09 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>https://tqzhong.github.io/my-blog/posts/blog1/</guid>
      <description>&lt;!-- 这篇主要从方法上梳理一下目前大模型后训练（post-training）的方法

### Bradley-Terry model

这是一个使用Bradley-Terry模型的示例公式：

$$
P(i &gt; j) = \frac{e^{\beta_i}}{e^{\beta_i} + e^{\beta_j}}
$$

其中 \( \beta_i \) 和 \( \beta_j \) 分别是两个实体的参数。


### RLHF

在强化学习从人类反馈(RLHF)中，我们常常使用以下形式的优化方程：

$$
\min_{\theta} \mathbb{E}\left[ \text{Loss}(\theta) \right]
$$

这里 \( \theta \) 表示模型参数。 --&gt;
&lt;p&gt;这是一个测试样例&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>

[{"content":"Introduction Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\nThere are two types of hallucination:\nIntrinsic Hallucination: This occurs when the model output contradicts or is inconsistent with the given input. Extrinsic Hallucination: This refers to outputs that are not grounded in external world knowledge. Why Hallucinations Matter Understanding hallucinations is critical when applying large language models in high-stakes areas like healthcare, law, and science. Inaccurate or fabricated information can have serious consequences.\n","permalink":"https://tqzhong.github.io/my-blog/posts/blog1/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eHallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and \u003cstrong\u003enot grounded\u003c/strong\u003e by either the provided context or world knowledge.\u003c/p\u003e\n\u003cp\u003eThere are two types of hallucination:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eIntrinsic Hallucination\u003c/strong\u003e: This occurs when the model output contradicts or is inconsistent with the given input.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eExtrinsic Hallucination\u003c/strong\u003e: This refers to outputs that are not grounded in external world knowledge.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"why-hallucinations-matter\"\u003eWhy Hallucinations Matter\u003c/h3\u003e\n\u003cp\u003eUnderstanding hallucinations is critical when applying large language models in high-stakes areas like healthcare, law, and science. Inaccurate or fabricated information can have serious consequences.\u003c/p\u003e","title":"Extrinsic Hallucinations in LLMs"},{"content":"Introduction Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\nThere are two types of hallucination:\nIntrinsic Hallucination: This occurs when the model output contradicts or is inconsistent with the given input. Extrinsic Hallucination: This refers to outputs that are not grounded in external world knowledge. Why Hallucinations Matter Understanding hallucinations is critical when applying large language models in high-stakes areas like healthcare, law, and science. Inaccurate or fabricated information can have serious consequences.\n","permalink":"https://tqzhong.github.io/my-blog/posts/blog2/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eHallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and \u003cstrong\u003enot grounded\u003c/strong\u003e by either the provided context or world knowledge.\u003c/p\u003e\n\u003cp\u003eThere are two types of hallucination:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eIntrinsic Hallucination\u003c/strong\u003e: This occurs when the model output contradicts or is inconsistent with the given input.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eExtrinsic Hallucination\u003c/strong\u003e: This refers to outputs that are not grounded in external world knowledge.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"why-hallucinations-matter\"\u003eWhy Hallucinations Matter\u003c/h3\u003e\n\u003cp\u003eUnderstanding hallucinations is critical when applying large language models in high-stakes areas like healthcare, law, and science. Inaccurate or fabricated information can have serious consequences.\u003c/p\u003e","title":"Extrinsic Research For LLMs"}]
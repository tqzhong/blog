[{"content":"Introduction Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\nThere are two types of hallucination:\nIntrinsic Hallucination: This occurs when the model output contradicts or is inconsistent with the given input. Extrinsic Hallucination: This refers to outputs that are not grounded in external world knowledge. Why Hallucinations Matter Understanding hallucinations is critical when applying large language models in high-stakes areas like healthcare, law, and science. Inaccurate or fabricated information can have serious consequences.\n","permalink":"https://tqzhong.github.io/my-blog/posts/blog1/","summary":"Introduction Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\nThere are two types of hallucination:\nIntrinsic Hallucination: This occurs when the model output contradicts or is inconsistent with the given input. Extrinsic Hallucination: This refers to outputs that are not grounded in external world knowledge. Why Hallucinations Matter Understanding hallucinations is critical when applying large language models in high-stakes areas like healthcare, law, and science. Inaccurate or fabricated information can have serious consequences.\n","title":"Extrinsic Hallucinations in LLMs"},{"content":"Introduction Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\nThere are two types of hallucination:\nIntrinsic Hallucination: This occurs when the model output contradicts or is inconsistent with the given input. Extrinsic Hallucination: This refers to outputs that are not grounded in external world knowledge. Why Hallucinations Matter Understanding hallucinations is critical when applying large language models in high-stakes areas like healthcare, law, and science. Inaccurate or fabricated information can have serious consequences.\n","permalink":"https://tqzhong.github.io/my-blog/posts/blog2/","summary":"Introduction Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\nThere are two types of hallucination:\nIntrinsic Hallucination: This occurs when the model output contradicts or is inconsistent with the given input. Extrinsic Hallucination: This refers to outputs that are not grounded in external world knowledge. Why Hallucinations Matter Understanding hallucinations is critical when applying large language models in high-stakes areas like healthcare, law, and science. Inaccurate or fabricated information can have serious consequences.\n","title":"Extrinsic Research For LLMs"}]
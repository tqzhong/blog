[{"content":"这篇主要从方法上梳理一下目前大模型后训练（post-training）的方法\nBradley-Terry model 这是一个使用Bradley-Terry模型的示例公式：\n$$ P(i \u0026gt; j) = \\frac{e^{\\beta_i}}{e^{\\beta_i} + e^{\\beta_j}} $$\n其中 ( \\beta_i ) 和 ( \\beta_j ) 分别是两个实体的参数。\nRLHF 在强化学习从人类反馈(RLHF)中，我们常常使用以下形式的优化方程：\n$$ \\min_{\\theta} \\mathbb{E}\\left[ \\text{Loss}(\\theta) \\right] $$\n这里 ( \\theta ) 表示模型参数。\n","permalink":"http://localhost:1313/my-blog/posts/blog1/","summary":"这篇主要从方法上梳理一下目前大模型后训练（post-training）的方法\nBradley-Terry model 这是一个使用Bradley-Terry模型的示例公式：\n$$ P(i \u0026gt; j) = \\frac{e^{\\beta_i}}{e^{\\beta_i} + e^{\\beta_j}} $$\n其中 ( \\beta_i ) 和 ( \\beta_j ) 分别是两个实体的参数。\nRLHF 在强化学习从人类反馈(RLHF)中，我们常常使用以下形式的优化方程：\n$$ \\min_{\\theta} \\mathbb{E}\\left[ \\text{Loss}(\\theta) \\right] $$\n这里 ( \\theta ) 表示模型参数。\n","title":"大模型post-training方法"}]